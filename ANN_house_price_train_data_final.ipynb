{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ANN_house_price_train_data.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynVQ0pWDnBAb",
        "outputId": "2e18684e-472a-44b8-c783-90bc5344e40f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqIE_yiqnGQR"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQALP6QbnSM0"
      },
      "source": [
        "df  = pd.read_csv('/content/drive/MyDrive/Python_google_colab/train_EDA_FS_file.csv')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "rH4K12apnYFd",
        "outputId": "8c8bf1f5-2c96-46ed-cbba-173ab0424235"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Id</th>\n",
              "      <th>SalePrice</th>\n",
              "      <th>MSZoning</th>\n",
              "      <th>Street</th>\n",
              "      <th>Alley</th>\n",
              "      <th>LotShape</th>\n",
              "      <th>LandContour</th>\n",
              "      <th>Utilities</th>\n",
              "      <th>LotConfig</th>\n",
              "      <th>LandSlope</th>\n",
              "      <th>Neighborhood</th>\n",
              "      <th>Condition1</th>\n",
              "      <th>Condition2</th>\n",
              "      <th>BldgType</th>\n",
              "      <th>HouseStyle</th>\n",
              "      <th>RoofStyle</th>\n",
              "      <th>RoofMatl</th>\n",
              "      <th>Exterior1st</th>\n",
              "      <th>Exterior2nd</th>\n",
              "      <th>MasVnrType</th>\n",
              "      <th>ExterQual</th>\n",
              "      <th>ExterCond</th>\n",
              "      <th>Foundation</th>\n",
              "      <th>BsmtQual</th>\n",
              "      <th>BsmtCond</th>\n",
              "      <th>BsmtExposure</th>\n",
              "      <th>BsmtFinType1</th>\n",
              "      <th>BsmtFinType2</th>\n",
              "      <th>Heating</th>\n",
              "      <th>HeatingQC</th>\n",
              "      <th>CentralAir</th>\n",
              "      <th>Electrical</th>\n",
              "      <th>KitchenQual</th>\n",
              "      <th>Functional</th>\n",
              "      <th>FireplaceQu</th>\n",
              "      <th>GarageType</th>\n",
              "      <th>GarageFinish</th>\n",
              "      <th>GarageQual</th>\n",
              "      <th>GarageCond</th>\n",
              "      <th>...</th>\n",
              "      <th>Fence</th>\n",
              "      <th>MiscFeature</th>\n",
              "      <th>SaleType</th>\n",
              "      <th>SaleCondition</th>\n",
              "      <th>MSSubClass</th>\n",
              "      <th>LotFrontage</th>\n",
              "      <th>LotArea</th>\n",
              "      <th>OverallQual</th>\n",
              "      <th>OverallCond</th>\n",
              "      <th>YearBuilt</th>\n",
              "      <th>YearRemodAdd</th>\n",
              "      <th>MasVnrArea</th>\n",
              "      <th>BsmtFinSF1</th>\n",
              "      <th>BsmtFinSF2</th>\n",
              "      <th>BsmtUnfSF</th>\n",
              "      <th>TotalBsmtSF</th>\n",
              "      <th>1stFlrSF</th>\n",
              "      <th>2ndFlrSF</th>\n",
              "      <th>LowQualFinSF</th>\n",
              "      <th>GrLivArea</th>\n",
              "      <th>BsmtFullBath</th>\n",
              "      <th>BsmtHalfBath</th>\n",
              "      <th>FullBath</th>\n",
              "      <th>HalfBath</th>\n",
              "      <th>BedroomAbvGr</th>\n",
              "      <th>KitchenAbvGr</th>\n",
              "      <th>TotRmsAbvGrd</th>\n",
              "      <th>Fireplaces</th>\n",
              "      <th>GarageYrBlt</th>\n",
              "      <th>GarageCars</th>\n",
              "      <th>GarageArea</th>\n",
              "      <th>WoodDeckSF</th>\n",
              "      <th>OpenPorchSF</th>\n",
              "      <th>EnclosedPorch</th>\n",
              "      <th>3SsnPorch</th>\n",
              "      <th>ScreenPorch</th>\n",
              "      <th>PoolArea</th>\n",
              "      <th>MiscVal</th>\n",
              "      <th>MoSold</th>\n",
              "      <th>YrSold</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>12.247694</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.208333</td>\n",
              "      <td>0.250</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.866667</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.8</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.235294</td>\n",
              "      <td>0.418208</td>\n",
              "      <td>0.366344</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.036765</td>\n",
              "      <td>0.098361</td>\n",
              "      <td>0.12250</td>\n",
              "      <td>0.125089</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.064212</td>\n",
              "      <td>0.140098</td>\n",
              "      <td>0.356155</td>\n",
              "      <td>0.413559</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.577712</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.375</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.046729</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.386460</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.111517</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>12.109011</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.8</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.495064</td>\n",
              "      <td>0.391317</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.875</td>\n",
              "      <td>0.227941</td>\n",
              "      <td>0.524590</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.173281</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.121575</td>\n",
              "      <td>0.206547</td>\n",
              "      <td>0.503056</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.470245</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.375</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.289720</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.324401</td>\n",
              "      <td>0.347725</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.363636</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>12.317167</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.208333</td>\n",
              "      <td>0.250</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.866667</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.8</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.235294</td>\n",
              "      <td>0.434909</td>\n",
              "      <td>0.422359</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.051471</td>\n",
              "      <td>0.114754</td>\n",
              "      <td>0.10125</td>\n",
              "      <td>0.086109</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.185788</td>\n",
              "      <td>0.150573</td>\n",
              "      <td>0.383441</td>\n",
              "      <td>0.419370</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.593095</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.375</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.065421</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.428773</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.076782</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.727273</td>\n",
              "      <td>0.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>11.849398</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.250</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.8</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.294118</td>\n",
              "      <td>0.388581</td>\n",
              "      <td>0.390295</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.669118</td>\n",
              "      <td>0.606557</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.038271</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.231164</td>\n",
              "      <td>0.123732</td>\n",
              "      <td>0.399941</td>\n",
              "      <td>0.366102</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.579157</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.375</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.416667</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.074766</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.452750</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.063985</td>\n",
              "      <td>0.492754</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>12.429216</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.625000</td>\n",
              "      <td>0.250</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.866667</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.8</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.235294</td>\n",
              "      <td>0.513123</td>\n",
              "      <td>0.468761</td>\n",
              "      <td>0.777778</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.058824</td>\n",
              "      <td>0.147541</td>\n",
              "      <td>0.21875</td>\n",
              "      <td>0.116052</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.209760</td>\n",
              "      <td>0.187398</td>\n",
              "      <td>0.466237</td>\n",
              "      <td>0.509927</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.666523</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.583333</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.074766</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.589563</td>\n",
              "      <td>0.224037</td>\n",
              "      <td>0.153565</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.50</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 82 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  Id  SalePrice  MSZoning  ...  PoolArea  MiscVal    MoSold  YrSold\n",
              "0           0   1  12.247694      0.75  ...       0.0      0.0  0.090909    0.50\n",
              "1           1   2  12.109011      0.75  ...       0.0      0.0  0.363636    0.25\n",
              "2           2   3  12.317167      0.75  ...       0.0      0.0  0.727273    0.50\n",
              "3           3   4  11.849398      0.75  ...       0.0      0.0  0.090909    0.00\n",
              "4           4   5  12.429216      0.75  ...       0.0      0.0  1.000000    0.50\n",
              "\n",
              "[5 rows x 82 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoSCfleona3B",
        "outputId": "6fa8be69-d277-4747-b0ff-314826c786f5"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1460, 82)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEwYTy6GnhWB"
      },
      "source": [
        "X = df.drop(['Id','SalePrice','Unnamed: 0'],axis =1)\n",
        "y = df['SalePrice']\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7zquOdynz_4"
      },
      "source": [
        "import torch\n",
        "\n",
        "X_train=torch.FloatTensor(X_train.values).cuda()\n",
        "X_test=torch.FloatTensor(X_test.values).cuda()\n",
        "y_train= torch.FloatTensor(y_train.values).cuda()\n",
        "y_test=torch.FloatTensor(y_test.values).cuda()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uggBJOQWn6kB",
        "outputId": "29eb7268-0eb7-48c8-8578-471a09acfd04"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1022, 79])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o795aLu3oFdc",
        "outputId": "e629a365-cae0-466a-f4b8-aafa245455ed"
      },
      "source": [
        "X_test.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([438, 79])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tp9ZbZEqonrl"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torchvision\n",
        "\n",
        "class ANN_House(nn.Module):\n",
        "  def __init__(self,input=79,hidden1=32,hidden2=8,hidden3 = 2,output=1):\n",
        "    super(ANN_House,self).__init__()\n",
        "    self.FC1 = nn.Linear(input,hidden1)\n",
        "    self.relu1 = nn.ReLU()\n",
        "    \n",
        "    self.FC2 = nn.Linear(hidden1,hidden2)\n",
        "    self.relu2 = nn.ReLU()\n",
        "    #self.dd1  = nn.Dropout(p=0.5)\n",
        "    self.FC3 = nn.Linear(hidden2,hidden3)\n",
        "    self.relu3 = nn.ReLU()\n",
        "    self.FC4 = nn.Linear(hidden3,output)\n",
        "  def forward(self,x):\n",
        "    x = self.FC1(x)\n",
        "    x = self.relu1(x)\n",
        "    #x = self.dd1\n",
        "    x = self.FC2(x)\n",
        "    x = self.relu2(x)\n",
        "    x = self.FC3(x)\n",
        "    x = self.relu3(x)\n",
        "    x = self.FC4(x)\n",
        "    #y_pred  = torch.sigmoid(x)\n",
        "    return x"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6pGsWtLpL-h",
        "outputId": "90d65860-e953-4cbe-e0ab-fa5fae2b9794"
      },
      "source": [
        "model1 = ANN_House()\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer1 = torch.optim.Adam(model1.parameters(), lr = 0.09)\n",
        "\n",
        "\n",
        "#from  torch.optim.lr_scheduler import LambdaLR\n",
        "\n",
        "# Assuming optimizer has two groups.\n",
        "\n",
        "#lambda2  = lambda epoch: 0.65 * epoch\n",
        "#scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer1, lr_lambda= lambda2)\n",
        "\n",
        "model1  = model1.cuda()\n",
        "\n",
        "print(model1.parameters)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<bound method Module.parameters of ANN_House(\n",
            "  (FC1): Linear(in_features=79, out_features=32, bias=True)\n",
            "  (relu1): ReLU()\n",
            "  (FC2): Linear(in_features=32, out_features=8, bias=True)\n",
            "  (relu2): ReLU()\n",
            "  (FC3): Linear(in_features=8, out_features=2, bias=True)\n",
            "  (relu3): ReLU()\n",
            "  (FC4): Linear(in_features=2, out_features=1, bias=True)\n",
            ")>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LDzuAkGpiQG",
        "outputId": "ef386dd2-0308-457d-d269-bc1955013ba0"
      },
      "source": [
        "\n",
        "epochs = 800\n",
        "final_losses=[]\n",
        "for i in range(epochs):\n",
        "  y_pred1 = model1.forward(X_train)\n",
        "  \n",
        "  optimizer1.zero_grad()\n",
        "  #l1 = loss_fn(y_pred1,y_train.reshape(-1,1))\n",
        "  l1 = loss_fn(y_pred1,y_train)\n",
        "  if i%10 == 0:\n",
        "    print(\"Epoch number: {} and the loss : {}\".format(i,l1))\n",
        "  l1.backward()\n",
        "  final_losses.append(l1)\n",
        "  optimizer1.step()\n",
        "  #scheduler.step()"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([1022])) that is different to the input size (torch.Size([1022, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch number: 0 and the loss : 151.20582580566406\n",
            "Epoch number: 10 and the loss : 129.09622192382812\n",
            "Epoch number: 20 and the loss : 109.77912902832031\n",
            "Epoch number: 30 and the loss : 92.47364807128906\n",
            "Epoch number: 40 and the loss : 77.20132446289062\n",
            "Epoch number: 50 and the loss : 63.89265823364258\n",
            "Epoch number: 60 and the loss : 52.42188262939453\n",
            "Epoch number: 70 and the loss : 42.6346321105957\n",
            "Epoch number: 80 and the loss : 34.36540222167969\n",
            "Epoch number: 90 and the loss : 27.447397232055664\n",
            "Epoch number: 100 and the loss : 21.718015670776367\n",
            "Epoch number: 110 and the loss : 17.022184371948242\n",
            "Epoch number: 120 and the loss : 13.21458625793457\n",
            "Epoch number: 130 and the loss : 10.161164283752441\n",
            "Epoch number: 140 and the loss : 7.740151882171631\n",
            "Epoch number: 150 and the loss : 5.842732906341553\n",
            "Epoch number: 160 and the loss : 4.373122692108154\n",
            "Epoch number: 170 and the loss : 3.2484395503997803\n",
            "Epoch number: 180 and the loss : 2.3981196880340576\n",
            "Epoch number: 190 and the loss : 1.7630641460418701\n",
            "Epoch number: 200 and the loss : 1.2946034669876099\n",
            "Epoch number: 210 and the loss : 0.9532979130744934\n",
            "Epoch number: 220 and the loss : 0.707714319229126\n",
            "Epoch number: 230 and the loss : 0.533208429813385\n",
            "Epoch number: 240 and the loss : 0.41075399518013\n",
            "Epoch number: 250 and the loss : 0.32589811086654663\n",
            "Epoch number: 260 and the loss : 0.26783487200737\n",
            "Epoch number: 270 and the loss : 0.22860294580459595\n",
            "Epoch number: 280 and the loss : 0.2024306356906891\n",
            "Epoch number: 290 and the loss : 0.185191810131073\n",
            "Epoch number: 300 and the loss : 0.17398148775100708\n",
            "Epoch number: 310 and the loss : 0.16678552329540253\n",
            "Epoch number: 320 and the loss : 0.16222602128982544\n",
            "Epoch number: 330 and the loss : 0.15937453508377075\n",
            "Epoch number: 340 and the loss : 0.15761491656303406\n",
            "Epoch number: 350 and the loss : 0.1565435826778412\n",
            "Epoch number: 360 and the loss : 0.15590006113052368\n",
            "Epoch number: 370 and the loss : 0.15551871061325073\n",
            "Epoch number: 380 and the loss : 0.15529584884643555\n",
            "Epoch number: 390 and the loss : 0.15516749024391174\n",
            "Epoch number: 400 and the loss : 0.15509460866451263\n",
            "Epoch number: 410 and the loss : 0.15505380928516388\n",
            "Epoch number: 420 and the loss : 0.15503133833408356\n",
            "Epoch number: 430 and the loss : 0.1550191193819046\n",
            "Epoch number: 440 and the loss : 0.1550126075744629\n",
            "Epoch number: 450 and the loss : 0.1550091952085495\n",
            "Epoch number: 460 and the loss : 0.15500742197036743\n",
            "Epoch number: 470 and the loss : 0.1550065279006958\n",
            "Epoch number: 480 and the loss : 0.15500608086585999\n",
            "Epoch number: 490 and the loss : 0.15500584244728088\n",
            "Epoch number: 500 and the loss : 0.15500575304031372\n",
            "Epoch number: 510 and the loss : 0.15500569343566895\n",
            "Epoch number: 520 and the loss : 0.15500567853450775\n",
            "Epoch number: 530 and the loss : 0.15500566363334656\n",
            "Epoch number: 540 and the loss : 0.15500564873218536\n",
            "Epoch number: 550 and the loss : 0.15500566363334656\n",
            "Epoch number: 560 and the loss : 0.15500566363334656\n",
            "Epoch number: 570 and the loss : 0.15500564873218536\n",
            "Epoch number: 580 and the loss : 0.15500564873218536\n",
            "Epoch number: 590 and the loss : 0.15500564873218536\n",
            "Epoch number: 600 and the loss : 0.15500564873218536\n",
            "Epoch number: 610 and the loss : 0.15500564873218536\n",
            "Epoch number: 620 and the loss : 0.15500564873218536\n",
            "Epoch number: 630 and the loss : 0.15500564873218536\n",
            "Epoch number: 640 and the loss : 0.15500564873218536\n",
            "Epoch number: 650 and the loss : 0.15500564873218536\n",
            "Epoch number: 660 and the loss : 0.15500564873218536\n",
            "Epoch number: 670 and the loss : 0.15500564873218536\n",
            "Epoch number: 680 and the loss : 0.15500564873218536\n",
            "Epoch number: 690 and the loss : 0.15500564873218536\n",
            "Epoch number: 700 and the loss : 0.15500564873218536\n",
            "Epoch number: 710 and the loss : 0.15500564873218536\n",
            "Epoch number: 720 and the loss : 0.15500564873218536\n",
            "Epoch number: 730 and the loss : 0.15500564873218536\n",
            "Epoch number: 740 and the loss : 0.15500564873218536\n",
            "Epoch number: 750 and the loss : 0.15500564873218536\n",
            "Epoch number: 760 and the loss : 0.15500564873218536\n",
            "Epoch number: 770 and the loss : 0.15500564873218536\n",
            "Epoch number: 780 and the loss : 0.15500564873218536\n",
            "Epoch number: 790 and the loss : 0.15500564873218536\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "DwmZBbkRpoW1",
        "outputId": "4bcf1e1d-9dd5-4209-bd20-cc9fef77d31a"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.plot(range(epochs),final_losses)\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Epoch')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYuDpop_p1WW"
      },
      "source": [
        "a = np.sqrt(0.15500564873218536)"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrKqvZfir9gu",
        "outputId": "fdfbb615-97d5-4ce7-e64b-a3f6cb5ee5b0"
      },
      "source": [
        "print('Root mean squared error is :',a)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Root mean squared error is : 0.3937075675322807\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IS844QSruhOu"
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LeakyReLU,PReLU,ELU\n",
        "from keras.layers import Dropout\n",
        "X = df.drop(['Id','SalePrice','Unnamed: 0'],axis =1)\n",
        "y = df['SalePrice']\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "a_train, a_test, b_train, b_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialising the ANN\n",
        "classifier = Sequential()\n",
        "\n",
        "\n",
        "# Adding the input layer and the first hidden layer\n",
        "classifier.add(Dense(units = 108, kernel_initializer  = 'he_uniform',activation='relu',input_dim  = 79))\n",
        "\n",
        "# Adding the second hidden layer\n",
        "classifier.add(Dense(units = 54, kernel_initializer  = 'he_uniform',activation='relu',input_dim  =  108))\n",
        "#classifier.add(Dropout(0.4))\n",
        "# Adding the third hidden layer\n",
        "classifier.add(Dense(units = 27, kernel_initializer  = 'he_uniform',activation='relu',input_dim  = 54))\n",
        "#classifier.add(Dropout(0.4))\n",
        "\n",
        "classifier.add(Dense(units = 13, kernel_initializer  = 'he_uniform',activation='relu',input_dim  = 27))\n",
        "#classifier.add(Dropout(0.4))\n",
        "classifier.add(Dense(units = 6, kernel_initializer  = 'he_uniform',activation='relu',input_dim  = 13))\n",
        "# Adding the output layer\n",
        "classifier.add(Dense(units = 1, kernel_initializer  = 'he_uniform'))\n",
        "\n",
        "opt = keras.optimizers.Adam(learning_rate=0.05)\n",
        "classifier.compile(loss=root_mean_squared_error, optimizer=opt)\n",
        "# Compiling the ANN\n",
        "#classifier.compile(loss=root_mean_squared_error, optimizer='Adamax')\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 329,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g914DcQZ9pw5",
        "outputId": "056796ee-5e05-40b8-8e83-f0a198ba9724"
      },
      "source": [
        "print(classifier.summary())"
      ],
      "execution_count": 289,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_44\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_167 (Dense)            (None, 108)               8640      \n",
            "_________________________________________________________________\n",
            "dense_168 (Dense)            (None, 54)                5886      \n",
            "_________________________________________________________________\n",
            "dense_169 (Dense)            (None, 27)                1485      \n",
            "_________________________________________________________________\n",
            "dense_170 (Dense)            (None, 13)                364       \n",
            "_________________________________________________________________\n",
            "dense_171 (Dense)            (None, 6)                 84        \n",
            "_________________________________________________________________\n",
            "dense_172 (Dense)            (None, 1)                 7         \n",
            "=================================================================\n",
            "Total params: 16,466\n",
            "Trainable params: 16,466\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkjvC0c_uq_7",
        "outputId": "e23503c3-eb7b-4b43-961c-bf645d4432de"
      },
      "source": [
        "# Fitting the ANN to the Training set\n",
        "model_history=classifier.fit(a_train, b_train,validation_split=0.3, batch_size = 10, epochs = 15)"
      ],
      "execution_count": 330,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "72/72 [==============================] - 1s 8ms/step - loss: 7.4893 - val_loss: 0.4379\n",
            "Epoch 2/15\n",
            "72/72 [==============================] - 0s 3ms/step - loss: 0.7526 - val_loss: 0.4149\n",
            "Epoch 3/15\n",
            "72/72 [==============================] - 0s 3ms/step - loss: 0.5389 - val_loss: 1.3222\n",
            "Epoch 4/15\n",
            "72/72 [==============================] - 0s 3ms/step - loss: 0.8761 - val_loss: 0.9678\n",
            "Epoch 5/15\n",
            "72/72 [==============================] - 0s 3ms/step - loss: 0.5800 - val_loss: 0.3375\n",
            "Epoch 6/15\n",
            "72/72 [==============================] - 0s 3ms/step - loss: 0.4071 - val_loss: 0.6110\n",
            "Epoch 7/15\n",
            "72/72 [==============================] - 0s 3ms/step - loss: 0.6013 - val_loss: 0.5275\n",
            "Epoch 8/15\n",
            "72/72 [==============================] - 0s 3ms/step - loss: 0.4207 - val_loss: 0.5060\n",
            "Epoch 9/15\n",
            "72/72 [==============================] - 0s 3ms/step - loss: 0.5672 - val_loss: 0.1535\n",
            "Epoch 10/15\n",
            "72/72 [==============================] - 0s 3ms/step - loss: 0.2393 - val_loss: 0.2434\n",
            "Epoch 11/15\n",
            "72/72 [==============================] - 0s 3ms/step - loss: 0.1914 - val_loss: 0.1307\n",
            "Epoch 12/15\n",
            "72/72 [==============================] - 0s 3ms/step - loss: 0.2424 - val_loss: 0.1612\n",
            "Epoch 13/15\n",
            "72/72 [==============================] - 0s 3ms/step - loss: 0.2237 - val_loss: 0.2314\n",
            "Epoch 14/15\n",
            "72/72 [==============================] - 0s 3ms/step - loss: 0.2162 - val_loss: 0.1361\n",
            "Epoch 15/15\n",
            "72/72 [==============================] - 0s 3ms/step - loss: 0.1954 - val_loss: 0.1248\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhQug0OUwdcr"
      },
      "source": [
        "from keras import backend as K\n",
        "def root_mean_squared_error(y_true, y_pred):\n",
        "        return K.sqrt(K.mean(K.square(y_pred - y_true)))"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjl2Lo4HxSVT",
        "outputId": "384ea5b2-69e9-427a-abbe-0ed576c9b364"
      },
      "source": [
        "y_pred1 = classifier.predict(a_test)\n",
        "from sklearn.metrics import accuracy_score,mean_squared_error\n",
        "print('MSE:', mean_squared_error(b_test,y_pred1))\n",
        "score=np.sqrt(mean_squared_error(b_test, y_pred1))\n",
        "print(\"RMSE:\" ,score)"
      ],
      "execution_count": 331,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MSE: 0.022490550466763327\n",
            "RMSE: 0.14996849824800984\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFsVurQoB_Da"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}